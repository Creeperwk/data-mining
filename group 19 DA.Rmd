---
title: "Data Mining Group 19"
author: "Dongxu Li, Kuan Wang, Anwen Jin, Xuanming Zhang"
output:
  pdf_document:
          latex_engine: xelatex
          number_sections: yes
fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)

```

```{r library, echo=FALSE,warning=FALSE,message=FALSE}
library(tidyverse)
library(ggplot2)
library(scales)
library(dplyr)
library(kableExtra)
library(gridExtra)
library(grid)
library(MASS)
library(class)
library(broom)
library(pROC)
library(readr)
library(plyr)
library(skimr)
library(knitr)
library(randomForest)
library(ROCR)
library(rpart)
library(rpart.plot)
library(e1071)
library(neuralnet)
library(NeuralNetTools)
library(adabag)
library(tidytext)
```

# Introduction

The group 19's work aims to investigate whether the client will subscribe the term deposit with 10,000 records from bank marketing dataset. There are 20 parameters that will affect people's wish. Due to the high colinearities between some variables in the same field, we select part of them from each four fields (bank client data, related with the last contact of the current campaign, other attributes, and social and economic context attributes). 

Our work is made up with three parts. First, we try to select approximate parameters to build following models, and this will be done by making plots for parameters to see if they have significant differences in success. In this part, age, marital, loan, duration and cons.price.idx are selected. Then, we use 8 methods (adaptive boosting, decision tree, bagging, random forest, neural network, LDA, KNN, QDA) to forecast the number that client will subscribe or not and compare them with the true consequences. Finally, by comparing the prediction results from 8 methods, the premium model is selected by choosing the optimal outcome.

```{r dataset,echo=FALSE}
#dataset
group_19 <- read_csv("group_19.csv")
```

# Data selection

## Discarded variables
Before data cleaning, we need to choose parameters. Few parameters' plots are shown below to illustrate that there are slightly differences in proportion when things occur in different choices. In figure 1, though clients have housing loan or not, the figure does not show significant difference which indicates that it is useless for predicting. 

```{r table for raw variables, eval = TRUE,echo=FALSE}
group_19[,1:9] %>%
  head()%>%
  kable(caption = '\\label{tab:summariesskim} A brief on data pt.1.',booktabs = TRUE, format = "latex", digits = 2) %>%
  kable_styling(font_size = 10, latex_options = "HOLD_position")

group_19[,10:17] %>%
  head()%>%
  kable(caption = '\\label{tab:summariesskim} A brief on data pt.2.',booktabs = TRUE, format = "latex", digits = 2) %>%
  kable_styling(font_size = 10, latex_options = "HOLD_position")

group_19[,18:21] %>%
  head()%>%
  kable(caption = '\\label{tab:summariesskim} A brief on data pt.3.',booktabs = TRUE, format = "latex", digits = 2) %>%
  kable_styling(font_size = 10, latex_options = "HOLD_position")
```

```{r,echo=FALSE,fig.align="center",out.width="80%", fig.pos="H",fig.cap="Barchart of Housing loan and Number of clients"}
p1<-ggplot(group_19, aes(housing, fill=y)) + geom_bar() +
  xlab("Housing Loan") + ylab("Number of Clients") +
  ggtitle("Number of clients with a housing loan or not") +
  scale_fill_discrete(name = "", labels = c("Faliure", "Success")) +
  theme(plot.title = element_text(hjust = 0.5, size = 10))
```

In addition, just like housing loan, the credit in default also shows the same consequence. In this way, parameters like these two do not have significant difference in proportion so that they can not supply enough information. In this way, such parameters we choose to exclude.


```{r,echo=FALSE,fig.pos="H",fig.align="center",out.width="80%",fig.cap="Barchart of Housing Loan and Credit of Default"}
p2<-ggplot(group_19, aes(default, fill=y)) + geom_bar() +
  xlab("Credit in Default") + ylab("Number of Clients") +
  ggtitle("Number of clients with credit in default") +
  scale_fill_discrete(name = "", labels = c("Faliure", "Success")) +
  theme(plot.title = element_text(hjust = 0.5, size = 10))
  grid.arrange(p1,p2, ncol=2)
```

Besides that, some parameters seem to be useless which only contain no and NA, so we choose to reject them, and for the rest parameters, we randomly choose part of them, and finally, the parameters age, marital, loan, duration and cons.price.idx are left for the following work.

## Data standardisation

These variables are chosen because of their clear trend and representative values. Since different variables have significant difference in range and absolute values, there might be some bias in proceeding different methods on these variables, especially for Neural network. In order to keep the data consistant in all expreiencing methods, categorical variables will be factorised and split into different dummy variables by using one-hot-encoding. Numerical variables that exceed (0~1) will be standardised and the result will be shown as follow. After we did the standardisation, we randomly split the data into three different groups: 50% for training, 25% for testing and 25% for validation.

```{r data cleaning,echo=FALSE,echo=FALSE}
min.max.scale<- function(x){
  (x-min(x))/(max(x)-min(x))
}

data <- na.omit(group_19) %>%
  dplyr::select(y,age,marital,loan,duration,cons.price.idx) %>%
  mutate(loan=as.factor(loan),y=as.factor(y)) %>%
  as.data.frame()

data <- cbind(data,model.matrix(~marital-1, data=data))

data <- data[,-3] %>%
  mutate_if(.predicate=is.numeric,
            .funs=min.max.scale)%>%
  as.data.frame()

data$loan<-ifelse(data$loan=="no",0,1)
```


```{r summary table for normalized variables, eval = TRUE,echo=FALSE}

my_skim <- skim_with(numeric = sfl(hist = NULL), 
                    base = sfl(n = length))
my_skim(data)%>%
  transmute(Variable=skim_variable, n = n, Mean=numeric.mean, SD=numeric.sd,
            Min=numeric.p0, Median=numeric.p50,  Max=numeric.p100,
            IQR = numeric.p75-numeric.p50) %>%
  kable(caption = '\\label{tab:summariesskim} Summary statistics 
       on different variables.',booktabs = TRUE, format = "latex", digits = 2) %>%
  kable_styling(font_size = 10, latex_options = "HOLD_position")
```

```{r,echo=FALSE}
#split into train set and text set
set.seed(123)
n <- nrow(data)
ind1 <- sample(c(1:n),        floor(0.5*n)) 
ind2 <- sample(c(1:n)[-ind1], floor(0.25* n)) 
ind3 <- setdiff(c(1:n),c(ind1,ind2))

data.train <- data[ind1,]
data.valid <- data[ind2,]
data.test <- data[ind3,]
```

# Method selection
## KNN
KNN:
k-nearest neighbours is a nonparametric classification method. This means that a parametric model do not be assumed for the data or the classes, and thus donâ€™t have to worry about diagnostic tests for normality or the like. For each point looking at the k nearest labelled points to it, and predicting the class of this point to be the class that 9 the majority of its neighbours shares. When there is a tie, choosing one of these classes at random for our prediction and avoiding ties like this when there are two classes by choosing only odd numbers of k. However, for other problems where there are more than two classes, it is not necessarily choose k to guarantee avoidance of ties.
```{r 1,echo=FALSE, fig.align="center", fig.pos="H", fig.cap="Correct Classification Rate for different values of k"}
class.rate<-numeric(25)
for(k in 1:25) {
  pred.class <- knn(data.train[,-1], data.valid[,-1], data.train[,1], k=k)
  class.rate[k] <- sum(pred.class==data.valid[,1])/length(pred.class)
}
plot(c(1:25), class.rate, type="b",
     main="Correct classification rates on the validation data for a range of k",
     xlab="k",ylab="Correct Classification Rate",cex.main=0.7)

k.opt <- which.max(class.rate)

knn.pred <- knn(data.train[,-1], data.test[,-1], data.train[,1], k=k.opt)
table(data.test[,1],knn.pred)
```
This the correct rate on the validatation set of k values, it is clear that k=4 is the optimal in this case.

## LDA

LDA is based on the idea of maximizing the inter-class mean and minimizing the intra-class variance. The idea is to project the data in low dimensions and project the data in the same class as close as possible to the projection points and as far as possible from the centroids of the projection points of the data in different classes after projection.

```{r 2LDA,echo=FALSE,fig.align="center", fig.pos="H", fig.cap="Density graph two classes separated by lda"}
##LDA
lda <- lda(y~age+loan+duration+cons.price.idx, data=data.train)

lda.pred <- predict(lda,newdata= data.test)$class
table(data.test$y,lda.pred)

lda.pred.tr <- predict(lda)
dataset <- data.frame(Type=data.train$y, lda=lda.pred.tr$x)
ggplot(dataset, aes(x=LD1)) + 
  geom_density(aes(group=Type, colour=Type, fill=Type), alpha=0.3)
```
From the density graph, we can conclude that the linear discriminant is not that effective in sepereating the two classes and make correct predictions.

## QDA
QDA (Quadratic discriminant analysis) is a variant of LDA in which a single covariance matrix is estimated for each category of observations. QDA is particularly useful if it is known in advance that individual categories exhibit different covariances. the disadvantage of QDA is that it cannot be used as a dimensionality reduction technique.

```{r 3QDA, echo=FALSE}
##QDA
qda <- qda(y~age+loan+duration+cons.price.idx, data=data.train)

qda.pred<- predict(qda,newdata= data.test)$class
table(data.test$y,qda.pred)
```

## Bagging and random forest
The bagging method and random forests are generally applied in decision trees. Bagging, also known as Bootstrap aggregation, is an integrated learning method. Integration learning, in short, is the combination of multiple weak classifiers into strong classifiers. Bagging is the process of extracting subtraining sets from the original dataset by Bootstrap, then, modeling all subtraining sets, and finally voting decisions.
Random forest is actually a modification of the bagging method, which selects m predictor variables from all variables. Random refers to the random extraction of N subsample spaces using the self-help method, and such subsample spaces can be extracted for the data itself or for the feature variables. Forest refers to the training of N decision trees using the set of N subsamples to make predictions, and it is compared to a forest because it uses the prediction results of multiple trees to vote for the final classification.


```{r 45,echo=FALSE}
##Bagging and random forest
bagging<- randomForest(y~.,data = data.train,mtry=4,ntree=200)
rf <- randomForest(y~., data=data.train,ntree=200)

bagging.pred <- predict(bagging, data.test, type="class")
rf.pred <- predict(rf, data.test, type="class")

table(data.test$y,bagging.pred)
table(data.test$y,rf.pred)
```

```{r 56 plots,echo=FALSE,out.width="70%",fig.pos="H", fig.cap="Variable importance plots for random forest"}
varImpPlot(rf, main="Predicting success in data")
```
```{r 56 plot,echo=FALSE,out.width="70%",fig.pos="H", fig.cap="Variable importance plots for bagging"}
varImpPlot(bagging, main="Predicting success in data")
```
According to these variable importance plots, duration is the most significant factor, followed by consumer price index and age, two categorical variables have very few impact in success.


## Decision tree
Decision tree:
A decision tree is a machine learning method that is a tree structure (either binary or non-binary) in which each internal node represents a judgment on an attribute, each branch represents the output of a judgment result, and finally each leaf node represents a classification result.


```{r ,echo=FALSE}
##Trees
tree <- rpart(y~., data=data.train, method="class")

tree.pred <- predict(tree, newdata=data.test[,-1],type="class")
```

```{r 6,echo=FALSE,fig.align="center", fig.pos="H", fig.cap="CP plot of full tree", out.width="80%"}
#Full tree
set.seed(1)
full.tree <- rpart(y~., data=data.train, method="class",
                   control=rpart.control(minsplit=2,minbucket=1,maxdepth=30,cp=-1))
plotcp(full.tree)
```
We used the smallest tree strategy for prune the tree, which refers the greatest cp value under the dashed line. In this case, cp=0.011.

```{r, echo=FALSE,fig.align="center", fig.pos="H", fig.cap="Pruned Decision Tree",out.width="80%"}
tree.pruned <- prune(full.tree, cp=0.011)
rpart.plot(tree.pruned)

tree.pruned.pred <- predict(tree.pruned, newdata=data.test[,-1],type="class")
table(data.test$y, tree.pruned.pred)
```
Applied the smallest tree strategy, the tree is pruned to the smallest one whose xerror rate is smaller than the dashed line above, which is (0.93521+0.042216)=0.977426. This turns out to be the Tree 5.

## Neural Networks
Neural network can start from a single layer perceptron and combine each summation unit and activation unit as neurons to obtain a multilayer perceptron, i.e., a general neural network. Here, the first and last input and output are called input and output layers, respectively; the layers composed of internal neurons are called hidden layers, and it is assumed that there are t hidden layers with pt neurons in each layer, and the superscript indicates the number of hidden layers and the subscript indicates the location of neurons in the layer. In addition, each unit between the layers is connected to each other, so it is also called fully connected network.

```{r 7,echo=FALSE, fig.align="center", fig.pos="H", fig.cap="AIC, BIC, and cross entropy loss of the neural networks"}
##Neural Networks
nn1<-neuralnet(y~.,data=data.train,hidden=3,linear.output=F,err.fct = 'ce',
               likelihood=TRUE, threshold = 0.1)
nn2<-neuralnet(y~.,data=data.train,hidden=5,linear.output=F,err.fct = 'ce',
               likelihood=TRUE, threshold = 0.1)
nn3<-neuralnet(y~.,data=data.train,hidden=7,linear.output=F,err.fct = 'ce',
               likelihood=TRUE, threshold = 0.1)

nn.class <- tibble('Network' = rep(c("NN_3","NN_5", "NN_7"), each = 3),
                   'Metric' = rep(c('AIC', 'BIC','CE loss'), length.out=9),
                   'Value' = c(nn1$result.matrix[4,1],
                               nn1$result.matrix[5,1],
                               nn1$result.matrix[1,1],
                               nn2$result.matrix[4,1],
                               nn2$result.matrix[5,1],
                               nn2$result.matrix[1,1],
                               nn3$result.matrix[4,1],
                               nn3$result.matrix[5,1],
                               nn3$result.matrix[1,1]))
nn_ggplot <- nn.class %>%
  ggplot(aes(Network, Value, fill=Metric)) +
  geom_col(position = 'dodge')
nn_ggplot

nn<-nn2
plot(nn)

nn.prob<-predict(nn,newdata=data.test)
nn.pred<-ifelse(nn.prob[,2]>0.5,'yes','no')
table(data.test$y,nn.pred)
```
Since the smaller value of AIC and BIC indicates a better model,the second neural network with 1 hidden layer and 5 nodes seemed to be best, while BIC preferred the first neural network with 1 hidden layers and 3 nodes. The cross entropy loss agreed with AIC so we would probably choose the second neural network.

# Adaptive boosting:
Adaptive boosting algorithm is an iterative boosting algorithm. Firstly, this algorithm gives a certain weight (equal weights at first time) to all observations and builds several models with weak classifiers. Secondly, it selects the model with minimal points that are wrongly classified and uses this model to predict the result and compare with the true value. Next, it assigns higher weights to points that are wrongly classified. These points will play a more significant role in building the new model. Then, this algorithm builds a new model with the adjusted weights. There is a high probability that this new model will perform better in predicting. Continuously repeat the above process. It will keep training models until and unless a lower error is received. Finally, the best weak classifiers obtained from each training are merged together as the final decision classifier with different weights. Weak classifier with better performance in predicting obtains the greater weight.

The specific process is as follows:
1.	Train the current iterative optimal weak classifier
The optimal weak classifier is the weak classifier with the smallest error rate. The formula for calculating the error rate is:
$$e_m= \Sigma^{N}_{i=1}W_{m,i}I(G_m(x_i)\neq y_i)$$
Where m = 1, 2, ..., M, represents the m-th iteration. i represents the i-th sample. w is the sample weight. I indicates that the function takes a value of 1 or 0. When the expression in the parentheses of the I indicator function is true, the result of the I function is 1; when the expression in the parentheses of the I function is false, the result of the I function is 0. The weak classifier with the lowest error rate is taken as the optimal weak classifier for the current iteration.
In the first round of iterative calculation, the sample weight is initialized to one-half of the total number of samples.
2.	Calculate the weight of the optimal weak classifier
The weight of the strong and weak classifier is only related to the error rate of the weak classifier. The weight calculation formula of the weak classifier is as follows:
$$\alpha_m=\frac{1}{2}log(\dfrac{1-e_m}{e_m})$$
It can be seen that when the error rate is smaller, the alpha value will be larger, which means the weight of the weak classifier will increase. On the contrary, when the error rate is larger, the alpha value will be smaller, and the weight of the weak classifier will decrease. In this way, the weak classifier with high classification accuracy can play a greater role, and the weak classifier with low accuracy can be weakened.
3.	Update the sample weight according to the error rate
The update of the sample weight is only related to the current sample weight and the weight of the weak classifier. The sample weight update formula is as follows:
$$W_{m+1,i}=\dfrac{W_{m,i}}{z_m}exp(-\alpha_my_iG_m(x_i))$$
$$Z_m = \Sigma^{N}_{i=1}W_{m,i}exp(-\alpha_my_iG_m(x_i))$$
Where m = 1, 2, ..., M, represents the m-th iteration. i represents the i-th sample. w is the sample weight. alpha is the weight of the weak classifier. When the sample is correctly classified, the values of y and Gm are consistent, and the weight of the new sample becomes smaller; when the sample is misclassified, the values of y and Gm are inconsistent, and the weight of the new sample becomes larger. In this way, the weight of misclassified samples can be increased, so that they can be valued in the next iteration.
4.	Iteration termination condition
Repeat steps 1, 2, and 3 until the termination condition is reached. The termination condition is that the error rate of the strong classifier falls below the minimum error rate threshold or reaches the maximum number of iterations.

```{r 8,echo=FALSE}
##Adaptive boosting
boost<-boosting(y~.,data=data.train,mfinal =50)
boost.pred<-predict(boost,newdata = data.test)$class
table(data.test$y,boost.pred)
```



# Conclusion
We get 8 outcomes with 4 situations which contains predicting no with no for real, predicting no with yes for real,predicting yes with no for real, and predicting yes with yes for real. By calculating four indexes (accuracy, fl_score, precision, and recall), we can list and visualize
them into figure 6. From this figure, we can clearly observe that all of 8 methods have high accuracy; adaptive boosting and bagging have high fl_score while random forest is the last place; random forest and adaptive boosting get high score in precision, but LDA, KNN, and QDA perform worse in this step; bagging and adaptive boosting hold the highest position in recall with random forest holding the lowest. 

```{r,echo=FALSE}
#Accuracy, precision, recall, F1-score
binary.class.metric <- function(true,predict,positive_level){
  accuracy = mean(true==predict)
  precision = sum(true==positive_level & predict==positive_level)/sum(true==predict)
  recall = sum(true==positive_level & predict==positive_level)/sum(true==positive_level)
  fl_score = 2*precision*recall/(precision+recall)
  return(list(accuracy = accuracy,
              precision = precision,
              recall = recall,
              fl_score = fl_score))
}
```

```{r,include=FALSE,echo=FALSE}
knn.metric<-binary.class.metric(true=data.test$y,predict=knn.pred,positive_level='yes')
knn.metric

lda.metric<-binary.class.metric(true=data.test$y,predict=lda.pred,positive_level='yes')
lda.metric

qda.metric<-binary.class.metric(true=data.test$y,predict=qda.pred,positive_level='yes')
qda.metric

bagging.metric<-binary.class.metric(true=data.test$y,predict=bagging.pred,positive_level='yes')
bagging.metric

rf.metric<-binary.class.metric(true=data.test$y,predict=rf.pred,positive_level='yes')
rf.metric

tree.metric<-binary.class.metric(true=data.test$y,predict=tree.pred,positive_level='yes')
tree.metric


nn.metric<-binary.class.metric(true=data.test$y,predict=nn.pred,positive_level='yes')
nn.metric

boost.metric<-binary.class.metric(true=data.test$y,predict=boost.pred,positive_level='yes')
boost.metric
```

```{r,echo=FALSE,fig.align='center',fig.cap="The Value of 8 models",fig.pos="h"}
#visualization
bind_rows(unlist(knn.metric),
          unlist(lda.metric),
          unlist(qda.metric),
          unlist(bagging.metric),
          unlist(rf.metric),
          unlist(tree.metric),
          unlist(nn.metric),
          unlist(boost.metric))%>%
  mutate(model=c('KNN','LDA','QDA','Bagging','Random Forest','Decision Tree','Neural Network','Adaptive Boosting'))%>%
  pivot_longer(cols=-model,
               names_to = 'metric',
               values_to = 'value')%>%
  mutate(model = reorder_within(x = model,by = value,within = metric)) %>%
  ggplot(aes(x = model,y = value,fill = metric)) +
  geom_col() +
  scale_x_reordered() +
  facet_wrap(~metric,scales = 'free') +
  labs(x ='Model',
       y ='Value',
       fill = 'Model') +
  coord_flip() +
  theme_test() 
```

Basing on the list, it seems that adapting boosting has the greatest performance, cause it has high levels in four comparisons. However, when focusing on the value, like accuracy, we can see that all methods holds on a high grades around 0.8, but the recall, even the greatest one only gets merely around 0.4 grade. It can be explained that though these methods have satisfied abilities to predict the right answer, they lack of the abilities to predict right answer for the real true situation.