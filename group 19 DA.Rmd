---
title: "Data Mining Group 19"
author: "Dongxu Li, Kuan Wang, Anwen Jin, Xuanming Zhang"
output:
  pdf_document:
          latex_engine: xelatex
          number_sections: yes
fig_caption: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)

```

```{r library, echo=FALSE,warning=FALSE,message=FALSE}
library(tidyverse)
library(ggplot2)
library(scales)
library(dplyr)
library(kableExtra)
library(gridExtra)
library(grid)
library(MASS)
library(class)
library(broom)
library(pROC)
library(readr)
library(plyr)
library(skimr)
library(knitr)
library(randomForest)
library(ROCR)
library(rpart)
library(rpart.plot)
library(e1071)
library(neuralnet)
library(NeuralNetTools)
library(adabag)
library(tidytext)
```

# Introduction

The group 19's work aims to investigate whether the client will subscribe the term deposit with 10,000 records from bank marketing dataset. There are 20 parameters that will affect people's wish. Due to the high colinearities between some variables in the same field, we select part of them from each four fields (bank client data, related with the last contact of the current campaign, other attributes, and social and economic context attributes). 

Our work is made up with three parts. First, we try to select approximate parameters to build following models, and this will be done by making plots for parameters to see if they have significant differences in success. In this part, age, marital, loan, duration and cons.price.idx are selected. Then, we use 8 methods (adaptive boosting, decision tree, bagging, random forest, neural network, LDA, KNN, QDA) to forecast the number that client will subscribe or not and compare them with the true consequences. Finally, by comparing the prediction results from 8 methods, the premium model is selected by choosing the optimal outcome.

```{r dataset,echo=FALSE}
#dataset
group_19 <- read_csv("group_19.csv")
```

# Data selection

## Discarded variables
Before data cleaning, we need to choose parameters. Few parameters' plots are shown below to illustrate that there are slightly differences in proportion when things occur in different choices. In figure 1, though clients have housing loan or not, the figure does not show significant difference which indicates that it is useless for predicting. 

```{r table for raw variables, eval = TRUE,echo=FALSE}
group_19[,1:9] %>%
  head()%>%
  kable(caption = '\\label{tab:summariesskim} A brief on data pt.1.',booktabs = TRUE, format = "latex", digits = 2) %>%
  kable_styling(font_size = 10, latex_options = "HOLD_position")

group_19[,10:17] %>%
  head()%>%
  kable(caption = '\\label{tab:summariesskim} A brief on data pt.2.',booktabs = TRUE, format = "latex", digits = 2) %>%
  kable_styling(font_size = 10, latex_options = "HOLD_position")

group_19[,18:21] %>%
  head()%>%
  kable(caption = '\\label{tab:summariesskim} A brief on data pt.3.',booktabs = TRUE, format = "latex", digits = 2) %>%
  kable_styling(font_size = 10, latex_options = "HOLD_position")
```

```{r,echo=FALSE,fig.align="center",out.width="80%", fig.pos="H",fig.cap="Barchart of Housing loan and Number of clients"}
p1<-ggplot(group_19, aes(housing, fill=y)) + geom_bar() +
  xlab("Housing Loan") + ylab("Number of Clients") +
  ggtitle("Number of clients with a housing loan or not") +
  scale_fill_discrete(name = "", labels = c("Faliure", "Success")) +
  theme(plot.title = element_text(hjust = 0.5, size = 10))
```

In addition, just like housing loan, the credit in default also shows the same consequence. In this way, parameters like these two do not have significant difference in proportion so that they can not supply enough information. In this way, such parameters we choose to exclude.


```{r,echo=FALSE,fig.pos="H",fig.align="center",out.width="80%",fig.cap="Barchart of Housing Loan and Credit of Default"}
p2<-ggplot(group_19, aes(default, fill=y)) + geom_bar() +
  xlab("Credit in Default") + ylab("Number of Clients") +
  ggtitle("Number of clients with credit in default") +
  scale_fill_discrete(name = "", labels = c("Faliure", "Success")) +
  theme(plot.title = element_text(hjust = 0.5, size = 10))
  grid.arrange(p1,p2, ncol=2)
```

Besides that, some parameters seem to be useless which only contain no and NA, so we choose to reject them, and for the rest parameters, we randomly choose part of them, and finally, the parameters age, marital, loan, duration and cons.price.idx are left for the following work.

## Data standardisation

These variables are chosen because of their clear trend and representative values. Since different variables have significant difference in range and absolute values, there might be some bias in proceeding different methods on these variables, especially for Neural network. In order to keep the data consistant in all expreiencing methods, categorical variables will be factorised and split into different dummy variables by using one-hot-encoding. Numerical variables that exceed (0~1) will be standardised and the result will be shown as follow. After we did the standardisation, we randomly split the data into three different groups: 50% for training, 25% for testing and 25% for validation.

```{r data cleaning,echo=FALSE,echo=FALSE}
min.max.scale<- function(x){
  (x-min(x))/(max(x)-min(x))
}

data <- na.omit(group_19) %>%
  dplyr::select(y,age,marital,loan,duration,cons.price.idx) %>%
  mutate(loan=as.factor(loan),y=as.factor(y)) %>%
  as.data.frame()

data <- cbind(data,model.matrix(~marital-1, data=data))

data <- data[,-3] %>%
  mutate_if(.predicate=is.numeric,
            .funs=min.max.scale)%>%
  as.data.frame()

data$loan<-ifelse(data$loan=="no",0,1)
```


```{r summary table for normalized variables, eval = TRUE,echo=FALSE}

my_skim <- skim_with(numeric = sfl(hist = NULL), 
                    base = sfl(n = length))
my_skim(data)%>%
  transmute(Variable=skim_variable, n = n, Mean=numeric.mean, SD=numeric.sd,
            Min=numeric.p0, Median=numeric.p50,  Max=numeric.p100,
            IQR = numeric.p75-numeric.p50) %>%
  kable(caption = '\\label{tab:summariesskim} Summary statistics 
       on different variables.',booktabs = TRUE, format = "latex", digits = 2) %>%
  kable_styling(font_size = 10, latex_options = "HOLD_position")
```

```{r,echo=FALSE}
#split into train set and text set
set.seed(123)
n <- nrow(data)
ind1 <- sample(c(1:n),        floor(0.5*n)) 
ind2 <- sample(c(1:n)[-ind1], floor(0.25* n)) 
ind3 <- setdiff(c(1:n),c(ind1,ind2))

data.train <- data[ind1,]
data.valid <- data[ind2,]
data.test <- data[ind3,]
```

# Method selection
## KNN
KNN:
k-nearest neighbours is a nonparametric classification method. This means that a parametric model do not be assumed for the data or the classes, and thus don’t have to worry about diagnostic tests for normality or the like. For each point looking at the 𝑘 nearest labelled points to it, and predicting the class of this point to be the class that 9 the majority of its neighbours shares. When there is a tie, choosing one of these classes at random for our prediction and avoiding ties like this when there are two classes by choosing only odd numbers of 𝑘. However, for other problems where there are more than two classes, it is not necessarily choose 𝑘 to guarantee avoidance of ties.
```{r 1,echo=FALSE, fig.align="center", fig.pos="H", fig.cap="Correct Classification Rate for different values of k"}
class.rate<-numeric(25)
for(k in 1:25) {
  pred.class <- knn(data.train[,-1], data.valid[,-1], data.train[,1], k=k)
  class.rate[k] <- sum(pred.class==data.valid[,1])/length(pred.class)
}
plot(c(1:25), class.rate, type="b",
     main="Correct classification rates on the validation data for a range of k",
     xlab="k",ylab="Correct Classification Rate",cex.main=0.7)

k.opt <- which.max(class.rate)

knn.pred <- knn(data.train[,-1], data.test[,-1], data.train[,1], k=k.opt)
table(data.test[,1],knn.pred)
```


## LDA

LDA is based on the idea of maximizing the inter-class mean and minimizing the intra-class variance. The idea is to project the data in low dimensions and project the data in the same class as close as possible to the projection points and as far as possible from the centroids of the projection points of the data in different classes after projection.

```{r 2LDA,echo=FALSE}
##LDA
lda <- lda(y~age+loan+duration+cons.price.idx, data=data.train)

lda.pred <- predict(lda,newdata= data.test)$class
table(data.test$y,lda.pred)
```

## QDA
QDA (Quadratic discriminant analysis) is a variant of LDA in which a single covariance matrix is estimated for each category of observations. QDA is particularly useful if it is known in advance that individual categories exhibit different covariances. the disadvantage of QDA is that it cannot be used as a dimensionality reduction technique.

```{r 3QDA, echo=FALSE}
##QDA
qda <- qda(y~age+loan+duration+cons.price.idx, data=data.train)

qda.pred<- predict(qda,newdata= data.test)$class
table(data.test$y,qda.pred)
```

## Bagging and random forest
The bagging method and random forests are generally applied in decision trees. Bagging, also known as Bootstrap aggregation, is an integrated learning method. Integration learning, in short, is the combination of multiple weak classifiers into strong classifiers. Bagging is the process of extracting subtraining sets from the original dataset by Bootstrap, then, modeling all subtraining sets, and finally voting decisions.
Random forest is actually a modification of the bagging method, which selects m predictor variables from all variables. Random refers to the random extraction of N subsample spaces using the self-help method, and such subsample spaces can be extracted for the data itself or for the feature variables. Forest refers to the training of N decision trees using the set of N subsamples to make predictions, and it is compared to a forest because it uses the prediction results of multiple trees to vote for the final classification.


```{r 45,echo=FALSE}
##Bagging and random forest
bagging<- randomForest(y~.,data = data.train,mtry=4,ntree=200)
rf <- randomForest(y~., data=data.train,ntree=200)

bagging.pred <- predict(bagging, data.test, type="class")
rf.pred <- predict(rf, data.test, type="class")

table(data.test$y,bagging.pred)
table(data.test$y,rf.pred)
```

## Decision tree
Decision tree:
A decision tree is a machine learning method that is a tree structure (either binary or non-binary) in which each internal node represents a judgment on an attribute, each branch represents the output of a judgment result, and finally each leaf node represents a classification result.


```{r ,echo=FALSE}
##Trees
tree <- rpart(y~., data=data.train, method="class")

tree.pred <- predict(tree, newdata=data.test[,-1],type="class")
```

```{r 6,echo=FALSE,fig.align="center", fig.pos="H", fig.cap="CP plot of full tree", out.width="80%"}
#Full tree
set.seed(1)
full.tree <- rpart(y~., data=data.train, method="class",
                   control=rpart.control(minsplit=2,minbucket=1,maxdepth=30,cp=-1))
plotcp(full.tree)
```

```{r, echo=FALSE,fig.align="center", fig.pos="H", fig.cap="Pruned Decision Tree",out.width="80%"}
tree.pruned <- prune(full.tree, cp=0.011)
rpart.plot(tree.pruned)

tree.pruned.pred <- predict(tree.pruned, newdata=data.test[,-1],type="class")
table(data.test$y, tree.pruned.pred)
```

## Neural Networks
neural network can start from a single layer perceptron and combine each summation unit and activation unit as neurons to obtain a multilayer perceptron, i.e., a general neural network. Here, the first and last input and output are called input and output layers, respectively; the layers composed of internal neurons are called hidden layers, and it is assumed that there are t hidden layers with pt neurons in each layer, and the superscript indicates the number of hidden layers and the subscript indicates the location of neurons in the layer. In addition, each unit between the layers is connected to each other, so it is also called fully connected network.

```{r 7,echo=FALSE, fig.align="center", fig.pos="H", fig.cap="AIC, BIC, and cross entropy loss of the neural networks"}
##Neural Networks
nn1<-neuralnet(y~.,data=data.train,hidden=3,linear.output=F,err.fct = 'ce',
               likelihood=TRUE, threshold = 0.1)
nn2<-neuralnet(y~.,data=data.train,hidden=5,linear.output=F,err.fct = 'ce',
               likelihood=TRUE, threshold = 0.1)
nn3<-neuralnet(y~.,data=data.train,hidden=7,linear.output=F,err.fct = 'ce',
               likelihood=TRUE, threshold = 0.1)

nn.class <- tibble('Network' = rep(c("NN_3","NN_5", "NN_7"), each = 3),
                   'Metric' = rep(c('AIC', 'BIC','CE loss'), length.out=9),
                   'Value' = c(nn1$result.matrix[4,1],
                               nn1$result.matrix[5,1],
                               nn1$result.matrix[1,1],
                               nn2$result.matrix[4,1],
                               nn2$result.matrix[5,1],
                               nn2$result.matrix[1,1],
                               nn3$result.matrix[4,1],
                               nn3$result.matrix[5,1],
                               nn3$result.matrix[1,1]))
nn_ggplot <- nn.class %>%
  ggplot(aes(Network, Value, fill=Metric)) +
  geom_col(position = 'dodge')
nn_ggplot

nn<-nn2
plot(nn)

nn.prob<-predict(nn,newdata=data.test)
nn.pred<-ifelse(nn.prob[,2]>0.5,'yes','no')
table(data.test$y,nn.pred)
```

## Adaptive boosting:
Adaptive Boosting helps to reduce the error of any classification learning algorithm by sequentially turning many weak classifiers into one strong classifier.
For example, find a WEAK base algorithm A, and then use A to minimize the weight error. Algorithm A can use a single-level decision tree, and then by adding Adaptive Boosting, the algorithm will perform better and better.

```{r 8,echo=FALSE}
##Adaptive boosting
boost<-boosting(y~.,data=data.train,mfinal =50)
boost.pred<-predict(boost,newdata = data.test)$class
table(data.test$y,boost.pred)
```



# Conclusion
We get 8 outcomes with 4 situations which contains predicting no with no for real, predicting no with yes for real,predicting yes with no for real, and predicting yes with yes for real. By calculating four indexes (accuracy, fl_score, precision, and recall), we can list and visualize
them into figure 6. From this figure, we can clearly observe that all of 8 methods have high accuracy; adaptive boosting and bagging have high fl_score while random forest is the last place; random forest and adaptive boosting get high score in precision, but LDA, KNN, and QDA perform worse in this step; bagging and adaptive boosting hold the highest position in recall with random forest holding the lowest. 

Basing on the list, it seems that adapting boosting has the greatest performance, cause it has high levels in four comparisons. However, when focusing on the value, like accuracy, we can see that all methods holds on a high grades around 0.8, but the recall, even the greatest one only gets merely around 0.4 grade. It can be explained that though these methods have satisfied abilities to predict the right answer, they lack of the abilities to predict right answer for the real true situation.
```{r,echo=FALSE}
#Accuracy, precision, recall, F1-score
binary.class.metric <- function(true,predict,positive_level){
  accuracy = mean(true==predict)
  precision = sum(true==positive_level & predict==positive_level)/sum(true==predict)
  recall = sum(true==positive_level & predict==positive_level)/sum(true==positive_level)
  fl_score = 2*precision*recall/(precision+recall)
  return(list(accuracy = accuracy,
              precision = precision,
              recall = recall,
              fl_score = fl_score))
}
```

```{r,include=FALSE,echo=FALSE}
knn.metric<-binary.class.metric(true=data.test$y,predict=knn.pred,positive_level='yes')
knn.metric

lda.metric<-binary.class.metric(true=data.test$y,predict=lda.pred,positive_level='yes')
lda.metric

qda.metric<-binary.class.metric(true=data.test$y,predict=qda.pred,positive_level='yes')
qda.metric

bagging.metric<-binary.class.metric(true=data.test$y,predict=bagging.pred,positive_level='yes')
bagging.metric

rf.metric<-binary.class.metric(true=data.test$y,predict=rf.pred,positive_level='yes')
rf.metric

tree.metric<-binary.class.metric(true=data.test$y,predict=tree.pred,positive_level='yes')
tree.metric


nn.metric<-binary.class.metric(true=data.test$y,predict=nn.pred,positive_level='yes')
nn.metric

boost.metric<-binary.class.metric(true=data.test$y,predict=boost.pred,positive_level='yes')
boost.metric
```

```{r,echo=FALSE,fig.align='center',fig.cap="The Value of 8 models",fig.pos="h"}
#visualization
bind_rows(unlist(knn.metric),
          unlist(lda.metric),
          unlist(qda.metric),
          unlist(bagging.metric),
          unlist(rf.metric),
          unlist(tree.metric),
          unlist(nn.metric),
          unlist(boost.metric))%>%
  mutate(model=c('KNN','LDA','QDA','Bagging','Random Forest','Decision Tree','Neural Network','Adaptive Boosting'))%>%
  pivot_longer(cols=-model,
               names_to = 'metric',
               values_to = 'value')%>%
  mutate(model = reorder_within(x = model,by = value,within = metric)) %>%
  ggplot(aes(x = model,y = value,fill = metric)) +
  geom_col() +
  scale_x_reordered() +
  facet_wrap(~metric,scales = 'free') +
  labs(x ='Model',
       y ='Value',
       fill = 'Model') +
  coord_flip() +
  theme_test() 
```

